{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d370c583",
   "metadata": {},
   "source": [
    "# Bayesian Inference Modelling\n",
    "## Goal: estimation of both evidence and posterior in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf75d73",
   "metadata": {},
   "source": [
    "# 1. Matern kernel and Gaussian prior\n",
    "It is well-known that BQ modelling is limited to certain kernel mean.<br>\n",
    "The combination of Gaussian prior and Matern kernel has no analytical kernel mean.<br>\n",
    "We show our BASQ framework can perform BQ for arbitrary kernel and prior distribution via Nyström approximation.<br>\n",
    "The problems are the same with tutorial 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e7c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gpytorch\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.means import ZeroMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.constraints import Interval\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "\n",
    "def set_model(X, Y):\n",
    "    base_kernel = MaternKernel(nu=2.5) # Matern kernel\n",
    "    mean_module = ZeroMean()\n",
    "    covar_module = ScaleKernel(base_kernel)\n",
    "\n",
    "    # Set a GP model\n",
    "    train_Y = Y.view(-1).unsqueeze(1)\n",
    "    likelihood = GaussianLikelihood()\n",
    "    likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.Interval(1e-12, 1e-9))\n",
    "    model = SingleTaskGP(X, train_Y, likelihood=likelihood, mean_module=mean_module, covar_module=covar_module)\n",
    "    hypers = {'likelihood.noise_covar.noise': torch.tensor(1e-10)}\n",
    "    model.initialize(**hypers)\n",
    "    model.likelihood.raw_noise.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def optimise_model(model):\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return model\n",
    "\n",
    "def set_and_opt_gp(X, Y):\n",
    "    model = set_model(X, Y)\n",
    "    model = optimise_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019602c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SOBER._utils import TensorManager\n",
    "from SOBER.BASQ._basq import BASQ\n",
    "from BASQ.experiment.gmm import GMM\n",
    "#import warnings\n",
    "tm = TensorManager()\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "num_dim = 10  # Number of dimensions of the true likelihood to be estimated\n",
    "mu_pi = torch.zeros(num_dim).to(tm.device, tm.dtype)  # the mean vactor of Gaussian prior\n",
    "cov_pi = 2 * torch.eye(num_dim).to(tm.device, tm.dtype)  # the covariance matrix of Gaussian prior\n",
    "\n",
    "from SOBER._prior import Gaussian\n",
    "prior = Gaussian(mu_pi, cov_pi)\n",
    "true_likelihood = GMM(num_dim, mu_pi, cov_pi, tm.device)  # true likelihood to be estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c37a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 2           # number of initial guess\n",
    "n_iterations = 10    # number of iterations (batches)\n",
    "n_cand = 20000       # number of candidates\n",
    "n_nys = 500          # number of Nyström samples\n",
    "n_batch = 100        # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438c2bc",
   "metadata": {},
   "source": [
    "# Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f6b2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - overhead: 0.954 [s]  logMAE of Integral: -1.825   logKL of posterior: -8.284\n",
      "Iter 0 - overhead: 0.954 [s]  logMAE of Integral: -1.825   logKL of posterior: -8.284\n",
      "Iter 1 - overhead: 0.932 [s]  logMAE of Integral: -2.822   logKL of posterior: -9.585\n",
      "Iter 1 - overhead: 0.932 [s]  logMAE of Integral: -2.822   logKL of posterior: -9.585\n",
      "Iter 2 - overhead: 2.116 [s]  logMAE of Integral: -2.875   logKL of posterior: -9.697\n",
      "Iter 2 - overhead: 2.116 [s]  logMAE of Integral: -2.875   logKL of posterior: -9.697\n",
      "Iter 3 - overhead: 1.595 [s]  logMAE of Integral: -2.153   logKL of posterior: -8.630\n",
      "Iter 3 - overhead: 1.595 [s]  logMAE of Integral: -2.153   logKL of posterior: -8.630\n",
      "Iter 4 - overhead: 1.546 [s]  logMAE of Integral: -2.695   logKL of posterior: -8.812\n",
      "Iter 4 - overhead: 1.546 [s]  logMAE of Integral: -2.695   logKL of posterior: -8.812\n",
      "Iter 5 - overhead: 1.789 [s]  logMAE of Integral: -3.358   logKL of posterior: -9.171\n",
      "Iter 5 - overhead: 1.789 [s]  logMAE of Integral: -3.358   logKL of posterior: -9.171\n",
      "Iter 6 - overhead: 1.812 [s]  logMAE of Integral: -3.999   logKL of posterior: -9.450\n",
      "Iter 6 - overhead: 1.812 [s]  logMAE of Integral: -3.999   logKL of posterior: -9.450\n",
      "Iter 7 - overhead: 1.794 [s]  logMAE of Integral: -3.748   logKL of posterior: -9.503\n",
      "Iter 7 - overhead: 1.794 [s]  logMAE of Integral: -3.748   logKL of posterior: -9.503\n",
      "Iter 8 - overhead: 2.060 [s]  logMAE of Integral: -4.654   logKL of posterior: -10.360\n",
      "Iter 8 - overhead: 2.060 [s]  logMAE of Integral: -4.654   logKL of posterior: -10.360\n",
      "Iter 9 - overhead: 2.452 [s]  logMAE of Integral: -7.005   logKL of posterior: -9.661\n",
      "Iter 9 - overhead: 2.452 [s]  logMAE of Integral: -7.005   logKL of posterior: -9.661\n"
     ]
    }
   ],
   "source": [
    "Z_true = 1                             # true integral\n",
    "x_test = prior.sample(10000)           # test data for evaluating posterior using KL divergence\n",
    "\n",
    "torch.manual_seed(0)                    # fix random seed for reproducibility\n",
    "X = prior.sample(n_init)               # inital dataset X\n",
    "Y = true_likelihood(X).to(tm.dtype)    # initial guess Y\n",
    "basq = BASQ(n_cand, n_nys, prior)      # set up BASQ instance\n",
    "model = set_and_opt_gp(X, Y)           # set up the GP surroage model\n",
    "\n",
    "for ith_round in range(n_iterations):\n",
    "    tik = time.monotonic()\n",
    "    X_batch, _ = basq.batch_uncertainty_sampling(model, n_batch)  # run BASQ algorithm to select 100 batch points\n",
    "    tok = time.monotonic()\n",
    "    overhead = tok - tik               # overhead of batch query\n",
    "    \n",
    "    Y_batch = true_likelihood(X_batch) # parallel query to true likelihood function\n",
    "    X = torch.cat([X, X_batch])        # concatenate the observations for X\n",
    "    Y = torch.cat([Y, Y_batch])        # concatenate the observations for X\n",
    "    \n",
    "    # Evaluation for integral\n",
    "    model = set_and_opt_gp(X, Y)       # retrain GP model\n",
    "    integral_estimated = basq.quadrature(model, 500)  # integral estimation\n",
    "    logMAE = (Z_true - integral_estimated).abs()      # evaluate the estimated integral value to true one\n",
    "    # EZ, VZ = basq.full_quadrature(model, 500)       # You can estimate integral variance (but takes more time)\n",
    "    \n",
    "    # Evaluation for the posterior\n",
    "    KL = basq.KLdivergence(Z_true, x_test, true_likelihood, model)  # compute the KL divergence\n",
    "    print('Iter %d - overhead: %.3f [s]  logMAE of Integral: %.3f   logKL of posterior: %.3f' % (\n",
    "        ith_round, overhead, logMAE.log().item(), KL.log().item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797505e2",
   "metadata": {},
   "source": [
    "# 2. Spectral Mixture kernel and Gaussian prior\n",
    "Next, we will try spectral mixture kernel, which is also popular kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b181e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "class SpectralMixtureGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(SpectralMixtureGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=num_dim)\n",
    "        self.covar_module.initialize_from_data(train_x, train_y)\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def set_model(X, Y):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = SpectralMixtureGPModel(X, Y, likelihood)\n",
    "    likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.Interval(1e-12, 1e-9))\n",
    "    hypers = {'likelihood.noise_covar.noise': torch.tensor(1e-10)}\n",
    "    model.initialize(**hypers)\n",
    "    model.likelihood.raw_noise.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def optimise_model(model, X, Y):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    for i in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = -mll(output, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "def set_and_opt_gp(X, Y):\n",
    "    model = set_model(X, Y)\n",
    "    model = optimise_model(model, X, Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca66ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 2           # number of initial guess\n",
    "n_iterations = 10    # number of iterations (batches)\n",
    "n_cand = 10000       # number of candidates\n",
    "n_nys = 100          # number of Nyström samples\n",
    "n_batch = 100        # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ecd2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - overhead: 1.759 [s]  logMAE of Integral: 0.000   logKL of posterior: -4.910\n",
      "Iter 0 - overhead: 1.759 [s]  logMAE of Integral: 0.000   logKL of posterior: -4.910\n",
      "Iter 1 - overhead: 3.045 [s]  logMAE of Integral: 0.000   logKL of posterior: -2.329\n",
      "Iter 1 - overhead: 3.045 [s]  logMAE of Integral: 0.000   logKL of posterior: -2.329\n",
      "Iter 2 - overhead: 3.267 [s]  logMAE of Integral: 0.000   logKL of posterior: -0.584\n",
      "Iter 2 - overhead: 3.267 [s]  logMAE of Integral: 0.000   logKL of posterior: -0.584\n",
      "Iter 3 - overhead: 5.185 [s]  logMAE of Integral: -0.000   logKL of posterior: -4.794\n",
      "Iter 3 - overhead: 5.185 [s]  logMAE of Integral: -0.000   logKL of posterior: -4.794\n",
      "Iter 4 - overhead: 6.836 [s]  logMAE of Integral: -0.000   logKL of posterior: -3.105\n",
      "Iter 4 - overhead: 6.836 [s]  logMAE of Integral: -0.000   logKL of posterior: -3.105\n",
      "Iter 5 - overhead: 17.360 [s]  logMAE of Integral: -0.000   logKL of posterior: -4.014\n",
      "Iter 5 - overhead: 17.360 [s]  logMAE of Integral: -0.000   logKL of posterior: -4.014\n",
      "Iter 6 - overhead: 21.351 [s]  logMAE of Integral: -0.000   logKL of posterior: -5.142\n",
      "Iter 6 - overhead: 21.351 [s]  logMAE of Integral: -0.000   logKL of posterior: -5.142\n",
      "Iter 7 - overhead: 28.450 [s]  logMAE of Integral: 0.000   logKL of posterior: -4.190\n",
      "Iter 7 - overhead: 28.450 [s]  logMAE of Integral: 0.000   logKL of posterior: -4.190\n",
      "Iter 8 - overhead: 36.453 [s]  logMAE of Integral: -0.000   logKL of posterior: -5.406\n",
      "Iter 8 - overhead: 36.453 [s]  logMAE of Integral: -0.000   logKL of posterior: -5.406\n"
     ]
    }
   ],
   "source": [
    "Z_true = 1                             # true integral\n",
    "x_test = prior.sample(10000)           # test data for evaluating posterior using KL divergence\n",
    "\n",
    "torch.manual_seed(0)                    # fix random seed for reproducibility\n",
    "X = prior.sample(n_init)               # inital dataset X\n",
    "Y = true_likelihood(X).to(tm.dtype)    # initial guess Y\n",
    "basq = BASQ(n_cand, n_nys, prior)      # set up BASQ instance\n",
    "model = set_and_opt_gp(X, Y)           # set up the GP surroage model\n",
    "\n",
    "for ith_round in range(n_iterations):\n",
    "    tik = time.monotonic()\n",
    "    X_batch, _ = basq.batch_uncertainty_sampling(model, n_batch)  # run BASQ algorithm to select 100 batch points\n",
    "    tok = time.monotonic()\n",
    "    overhead = tok - tik               # overhead of batch query\n",
    "    \n",
    "    Y_batch = true_likelihood(X_batch) # parallel query to true likelihood function\n",
    "    X = torch.cat([X, X_batch])        # concatenate the observations for X\n",
    "    Y = torch.cat([Y, Y_batch])        # concatenate the observations for X\n",
    "    \n",
    "    # Evaluation for integral\n",
    "    model = set_and_opt_gp(X, Y)       # retrain GP model\n",
    "    integral_estimated = basq.quadrature(model, 100)  # integral estimation\n",
    "    logMAE = (Z_true - integral_estimated).abs()      # evaluate the estimated integral value to true one\n",
    "    # EZ, VZ = basq.full_quadrature(model, 500)       # You can estimate integral variance (but takes more time)\n",
    "    \n",
    "    # Evaluation for the posterior\n",
    "    KL = basq.KLdivergence(Z_true, x_test, true_likelihood, model)  # compute the KL divergence\n",
    "    print('Iter %d - overhead: %.3f [s]  logMAE of Integral: %.3f   logKL of posterior: %.3f' % (\n",
    "        ith_round, overhead, logMAE.log().item(), KL.log().item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380dd76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
