{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d370c583",
   "metadata": {},
   "source": [
    "# Bayesian Inference Modelling\n",
    "## Goal: estimation of both evidence and posterior in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf75d73",
   "metadata": {},
   "source": [
    "# 1. Design the Gaussian process surrogate model.\n",
    "- We model GP as zero mean GP with RBF kernel. (Vanilla BQ)\n",
    "- Optimize the hyperparameters based on type-II MLE using BoTorch optimizer (L-BFGS-B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e7c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gpytorch\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ZeroMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.constraints import Interval\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "\n",
    "def set_model(X, Y):\n",
    "    base_kernel = RBFKernel()\n",
    "    mean_module = ZeroMean()\n",
    "    covar_module = ScaleKernel(base_kernel)\n",
    "\n",
    "    # Set a GP model\n",
    "    train_Y = Y.view(-1).unsqueeze(1)\n",
    "    likelihood = GaussianLikelihood()\n",
    "    likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.Interval(1e-12, 1e-9))\n",
    "    model = SingleTaskGP(X, train_Y, likelihood=likelihood, mean_module=mean_module, covar_module=covar_module)\n",
    "    hypers = {'likelihood.noise_covar.noise': torch.tensor(1e-10)}\n",
    "    model.initialize(**hypers)\n",
    "    model.likelihood.raw_noise.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def optimise_model(model):\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return model\n",
    "\n",
    "def set_and_opt_gp(X, Y):\n",
    "    model = set_model(X, Y)\n",
    "    model = optimise_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6494af1",
   "metadata": {},
   "source": [
    "# 2. Set up the problems we wish to solve\n",
    "- true_likelihood: a likelihood modelled with Gaussian mixture. We wish to estimate this function only from the queries to this.\n",
    "- prior: a unimodal multivariate normal distribution. mean: mu_pi, covariance matrix: cov_pi\n",
    "- true evidence: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019602c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SOBER._utils import TensorManager\n",
    "from SOBER.BASQ._basq import BASQ\n",
    "from BASQ.experiment.gmm import GMM\n",
    "#import warnings\n",
    "tm = TensorManager()\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "num_dim = 10  # Number of dimensions of the true likelihood to be estimated\n",
    "mu_pi = torch.zeros(num_dim).to(tm.device, tm.dtype)  # the mean vactor of Gaussian prior\n",
    "cov_pi = 2 * torch.eye(num_dim).to(tm.device, tm.dtype)  # the covariance matrix of Gaussian prior\n",
    "\n",
    "from SOBER._prior import Gaussian\n",
    "prior = Gaussian(mu_pi, cov_pi)\n",
    "true_likelihood = GMM(num_dim, mu_pi, cov_pi, tm.device)  # true likelihood to be estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec928f1",
   "metadata": {},
   "source": [
    "# 3. Set up experimental condition\n",
    "We set up the experimental conditions\n",
    "- initial dataset: (X, Y) = (train_x, train_y). Generated from prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c37a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 2           # number of initial guess\n",
    "n_iterations = 10    # number of iterations (batches)\n",
    "n_cand = 20000       # number of candidates\n",
    "n_nys = 500          # number of Nystr√∂m samples\n",
    "n_batch = 100        # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438c2bc",
   "metadata": {},
   "source": [
    "# 4. Run!\n",
    "### 4.1 Metric for integral estimation\n",
    "- the KL divergence between true and estimated posterior.\n",
    "- posterior = E[GP-modelled-likelihood] * prior / marginal-likelihood\n",
    "\n",
    "### 4.2 Metric for posterior estimation\n",
    "- logarithmic mean absolute error between true and estimated evidence.\n",
    "- logMAE = (Z_estimated - Z_true).abs().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f6b2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - overhead: 0.612 [s]  logMAE of Integral: -2.464   logKL of posterior: -8.234\n",
      "Iter 1 - overhead: 0.683 [s]  logMAE of Integral: -4.188   logKL of posterior: -9.943\n",
      "Iter 2 - overhead: 1.085 [s]  logMAE of Integral: -3.324   logKL of posterior: -9.166\n",
      "Iter 3 - overhead: 1.186 [s]  logMAE of Integral: -4.013   logKL of posterior: -11.666\n",
      "Iter 4 - overhead: 1.130 [s]  logMAE of Integral: -2.723   logKL of posterior: -8.678\n",
      "Iter 5 - overhead: 1.103 [s]  logMAE of Integral: -3.592   logKL of posterior: -11.177\n",
      "Iter 6 - overhead: 1.156 [s]  logMAE of Integral: -4.558   logKL of posterior: -9.773\n",
      "Iter 7 - overhead: 1.221 [s]  logMAE of Integral: -4.137   logKL of posterior: -11.323\n",
      "Iter 8 - overhead: 1.391 [s]  logMAE of Integral: -4.951   logKL of posterior: -10.951\n",
      "Iter 9 - overhead: 1.680 [s]  logMAE of Integral: -3.583   logKL of posterior: -9.448\n"
     ]
    }
   ],
   "source": [
    "Z_true = 1                             # true integral\n",
    "x_test = prior.sample(10000)           # test data for evaluating posterior using KL divergence\n",
    "\n",
    "torch.manual_seed(0)                    # fix random seed for reproducibility\n",
    "X = prior.sample(n_init)               # inital dataset X\n",
    "Y = true_likelihood(X).to(tm.dtype)    # initial guess Y\n",
    "basq = BASQ(n_cand, n_nys, prior)      # set up BASQ instance\n",
    "model = set_and_opt_gp(X, Y)           # set up the GP surroage model\n",
    "\n",
    "for ith_round in range(n_iterations):\n",
    "    tik = time.monotonic()\n",
    "    X_batch, _ = basq.batch_uncertainty_sampling(model, n_batch)  # run BASQ algorithm to select 100 batch points\n",
    "    tok = time.monotonic()\n",
    "    overhead = tok - tik               # overhead of batch query\n",
    "    \n",
    "    Y_batch = true_likelihood(X_batch) # parallel query to true likelihood function\n",
    "    X = torch.cat([X, X_batch])        # concatenate the observations for X\n",
    "    Y = torch.cat([Y, Y_batch])        # concatenate the observations for X\n",
    "    \n",
    "    # Evaluation for integral\n",
    "    model = set_and_opt_gp(X, Y)       # retrain GP model\n",
    "    integral_estimated = basq.quadrature(model, 500)  # integral estimation\n",
    "    logMAE = (Z_true - integral_estimated).abs()      # evaluate the estimated integral value to true one\n",
    "    # EZ, VZ = basq.full_quadrature(model, 500)       # You can estimate integral variance (but takes more time)\n",
    "    \n",
    "    # Evaluation for the posterior\n",
    "    KL = basq.KLdivergence(Z_true, x_test, true_likelihood, model)  # compute the KL divergence\n",
    "    print('Iter %d - overhead: %.3f [s]  logMAE of Integral: %.3f   logKL of posterior: %.3f' % (\n",
    "        ith_round, overhead, logMAE.log().item(), KL.log().item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efc2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
