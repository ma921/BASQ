{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d370c583",
   "metadata": {},
   "source": [
    "# Bayesian Inference Modelling\n",
    "## Goal: estimation of both evidence and posterior in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf75d73",
   "metadata": {},
   "source": [
    "# 1. WSABI model\n",
    "- We use the WSABI-M model in the following paper.\n",
    "- Gunter T, Osborne MA, Garnett R, Hennig P, Roberts SJ. Sampling for inference in probabilistic models with fast Bayesian quadrature. Advances in neural information processing systems 27 (2014).\n",
    "- You can also easily try WSABI-L by changing label to \"wsabil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e7c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from BASQ._wsabi import WsabiGP\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "def set_and_opt_gp(X, Y):\n",
    "    kernel = ScaleKernel(RBFKernel())\n",
    "    model = WsabiGP(X, Y, kernel, tm.device, alpha_factor=1, label=\"wsabim\", optimiser=\"BoTorch\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019602c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SOBER._utils import TensorManager\n",
    "from SOBER.BASQ._basq import BASQ\n",
    "from BASQ.experiment.gmm import GMM\n",
    "tm = TensorManager()\n",
    "\n",
    "num_dim = 10  # Number of dimensions of the true likelihood to be estimated\n",
    "mu_pi = torch.zeros(num_dim).to(tm.device, tm.dtype)  # the mean vactor of Gaussian prior\n",
    "cov_pi = 2 * torch.eye(num_dim).to(tm.device, tm.dtype)  # the covariance matrix of Gaussian prior\n",
    "\n",
    "from SOBER._prior import Gaussian\n",
    "prior = Gaussian(mu_pi, cov_pi)\n",
    "true_likelihood = GMM(num_dim, mu_pi, cov_pi, tm.device)  # true likelihood to be estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c37a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 2           # number of initial guess\n",
    "n_iterations = 10    # number of iterations (batches)\n",
    "n_cand = 20000       # number of candidates\n",
    "n_nys = 500          # number of Nyström samples\n",
    "n_batch = 100        # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438c2bc",
   "metadata": {},
   "source": [
    "# Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f6b2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - overhead: 0.997 [s]  logMAE of Integral: -1.912   logKL of posterior: -7.398\n",
      "Iter 1 - overhead: 1.132 [s]  logMAE of Integral: -0.640   logKL of posterior: -8.998\n",
      "Iter 2 - overhead: 1.725 [s]  logMAE of Integral: -1.355   logKL of posterior: -8.720\n",
      "Iter 3 - overhead: 1.823 [s]  logMAE of Integral: -1.692   logKL of posterior: -9.039\n",
      "Iter 4 - overhead: 1.769 [s]  logMAE of Integral: -2.508   logKL of posterior: -8.729\n",
      "Iter 5 - overhead: 2.059 [s]  logMAE of Integral: -1.663   logKL of posterior: -10.367\n",
      "Iter 6 - overhead: 2.272 [s]  logMAE of Integral: -1.075   logKL of posterior: -9.662\n",
      "Iter 7 - overhead: 2.597 [s]  logMAE of Integral: -1.485   logKL of posterior: -11.935\n",
      "Iter 8 - overhead: 2.555 [s]  logMAE of Integral: -1.185   logKL of posterior: -9.623\n",
      "Iter 9 - overhead: 2.886 [s]  logMAE of Integral: -1.359   logKL of posterior: -10.310\n"
     ]
    }
   ],
   "source": [
    "Z_true = 1                             # true integral\n",
    "x_test = prior.sample(10000)           # test data for evaluating posterior using KL divergence\n",
    "\n",
    "torch.manual_seed(0)                    # fix random seed for reproducibility\n",
    "X = prior.sample(n_init)               # inital dataset X\n",
    "Y = true_likelihood(X).to(tm.dtype)    # initial guess Y\n",
    "\n",
    "# CAUTION! You need to specify the model is warped GP by making it True.\n",
    "basq = BASQ(n_cand, n_nys, prior, warped_gp=True)      # set up BASQ instance\n",
    "model = set_and_opt_gp(X, Y)           # set up the GP surroage model\n",
    "\n",
    "for ith_round in range(n_iterations):\n",
    "    tik = time.monotonic()\n",
    "    X_batch, _ = basq.batch_uncertainty_sampling(model, n_batch)  # run BASQ algorithm to select 100 batch points\n",
    "    tok = time.monotonic()\n",
    "    overhead = tok - tik               # overhead of batch query\n",
    "    \n",
    "    Y_batch = true_likelihood(X_batch) # parallel query to true likelihood function\n",
    "    X = torch.cat([X, X_batch])        # concatenate the observations for X\n",
    "    Y = torch.cat([Y, Y_batch])        # concatenate the observations for X\n",
    "    \n",
    "    # Evaluation for integral\n",
    "    model = set_and_opt_gp(X, Y)       # retrain GP model\n",
    "    integral_estimated = basq.quadrature(model, 500)  # integral estimation\n",
    "    logMAE = (Z_true - integral_estimated).abs()      # evaluate the estimated integral value to true one\n",
    "    # EZ, VZ = basq.full_quadrature(model, 500)       # You can estimate integral variance (but takes more time)\n",
    "    \n",
    "    # Evaluation for the posterior\n",
    "    KL = basq.KLdivergence(Z_true, x_test, true_likelihood, model)  # compute the KL divergence\n",
    "    print('Iter %d - overhead: %.3f [s]  logMAE of Integral: %.3f   logKL of posterior: %.3f' % (\n",
    "        ith_round, overhead, logMAE.log().item(), KL.log().item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c1829",
   "metadata": {},
   "source": [
    "# 2. MMLT model\n",
    "- We use the MMLT model in the following papers.\n",
    "- This allows **true \"log\" likelihood**. This is more natural for most Bayesian inference.\n",
    "- Thus, the observed values Y should be in log space\n",
    "- MMLT is good if likelihood takes very large dynamic range. Otherwise, non-warped GP works better.\n",
    "\n",
    "- Chai HR, Garnett R. Improving quadrature for constrained integrands. InThe 22nd International Conference on Artificial Intelligence and Statistics 2019 Apr 11 (pp. 2751-2759). PMLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6120e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from SOBER.BASQ._scale_mmlt import ScaleMmltGP\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "def set_and_opt_gp(X, Y):\n",
    "    kernel = ScaleKernel(RBFKernel())\n",
    "    model = ScaleMmltGP(X, Y, kernel, tm.device, optimiser=\"BoTorch\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56c01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SOBER._utils import TensorManager\n",
    "from SOBER.BASQ._basq import BASQ\n",
    "from BASQ.experiment.gmm import GMM\n",
    "tm = TensorManager()\n",
    "\n",
    "num_dim = 10  # Number of dimensions of the true likelihood to be estimated\n",
    "mu_pi = torch.zeros(num_dim).to(tm.device, tm.dtype)  # the mean vactor of Gaussian prior\n",
    "cov_pi = 2 * torch.eye(num_dim).to(tm.device, tm.dtype)  # the covariance matrix of Gaussian prior\n",
    "\n",
    "from SOBER._prior import Gaussian\n",
    "prior = Gaussian(mu_pi, cov_pi)\n",
    "true_likelihood = GMM(num_dim, mu_pi, cov_pi, tm.device)  # true likelihood to be estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61edf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 2           # number of initial guess\n",
    "n_iterations = 10    # number of iterations (batches)\n",
    "n_cand = 20000       # number of candidates\n",
    "n_nys = 500          # number of Nyström samples\n",
    "n_batch = 100        # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717af06",
   "metadata": {},
   "source": [
    "# Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535721ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - overhead: 1.181 [s]  logMAE of Integral: -0.202   logKL of posterior: -8.846\n",
      "Iter 1 - overhead: 1.441 [s]  logMAE of Integral: -0.080   logKL of posterior: -8.546\n",
      "Iter 2 - overhead: 1.276 [s]  logMAE of Integral: -0.046   logKL of posterior: -8.012\n",
      "Iter 3 - overhead: 1.386 [s]  logMAE of Integral: -0.040   logKL of posterior: -7.971\n",
      "Iter 4 - overhead: 1.792 [s]  logMAE of Integral: -0.046   logKL of posterior: -8.816\n",
      "Iter 5 - overhead: 1.521 [s]  logMAE of Integral: -0.044   logKL of posterior: -8.584\n",
      "Iter 6 - overhead: 1.697 [s]  logMAE of Integral: -0.047   logKL of posterior: -9.621\n",
      "Iter 7 - overhead: 1.861 [s]  logMAE of Integral: -0.047   logKL of posterior: -9.490\n",
      "Iter 8 - overhead: 2.386 [s]  logMAE of Integral: -0.022   logKL of posterior: -8.923\n",
      "Iter 9 - overhead: 2.747 [s]  logMAE of Integral: -0.021   logKL of posterior: -8.458\n"
     ]
    }
   ],
   "source": [
    "Z_true = 1                             # true integral\n",
    "x_test = prior.sample(10000)           # test data for evaluating posterior using KL divergence\n",
    "\n",
    "torch.manual_seed(0)                    # fix random seed for reproducibility\n",
    "X = prior.sample(n_init)               # inital dataset X\n",
    "# CAUTION! You need to give \"log\" likelihood\n",
    "Y = true_likelihood(X).log().to(tm.dtype)    # initial guess Y\n",
    "\n",
    "# CAUTION! You need to specify the model is warped GP by making it True.\n",
    "basq = BASQ(n_cand, n_nys, prior, warped_gp=True)      # set up BASQ instance\n",
    "model = set_and_opt_gp(X, Y)           # set up the GP surroage model\n",
    "\n",
    "for ith_round in range(n_iterations):\n",
    "    tik = time.monotonic()\n",
    "    X_batch, _ = basq.batch_uncertainty_sampling(model, n_batch)  # run BASQ algorithm to select 100 batch points\n",
    "    tok = time.monotonic()\n",
    "    overhead = tok - tik               # overhead of batch query\n",
    "    \n",
    "    Y_batch = true_likelihood(X_batch).log().to(tm.dtype) # parallel query to true likelihood function\n",
    "    X = torch.cat([X, X_batch])        # concatenate the observations for X\n",
    "    Y = torch.cat([Y, Y_batch])        # concatenate the observations for X\n",
    "    \n",
    "    # Evaluation for integral\n",
    "    model = set_and_opt_gp(X, Y)       # retrain GP model\n",
    "    integral_estimated = basq.quadrature(model, 500)  # integral estimation\n",
    "    logMAE = (Z_true - integral_estimated).abs()      # evaluate the estimated integral value to true one\n",
    "    # EZ, VZ = basq.full_quadrature(model, 500)       # You can estimate integral variance (but takes more time)\n",
    "    \n",
    "    # Evaluation for the posterior\n",
    "    KL = basq.KLdivergence(Z_true, x_test, true_likelihood, model)  # compute the KL divergence\n",
    "    print('Iter %d - overhead: %.3f [s]  logMAE of Integral: %.3f   logKL of posterior: %.3f' % (\n",
    "        ith_round, overhead, logMAE.log().item(), KL.log().item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efc2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
